{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib1jYjNtUCog"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import nest_asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "nest_asyncio.apply()\n",
        "\n",
        "HTML_SAVE_PATH = \"html_save/\" # Input the path you want to save all files into\n",
        "DATABASE_URL = \"https://www.wise-agent.com/database/\" # Input the url you want to scrape\n",
        "DATABASE_NAME = \"wise-agent\" # Set name of database for scraping\n",
        "ENCODING_FORMAT = \"utf-8\"\n",
        "MAIN_CONTENT_CLASS = \"main-content-box clearfix\" # The class of main content of target\n",
        "MAIN_CONTENT_ID = \"\"\n",
        "\n",
        "async def get_html_of_database(DATABASE_URL, DATABASE_NAME) -> None:\n",
        "    async with async_playwright() as playwright:\n",
        "        browser = await playwright.webkit.launch(headless=True)\n",
        "        context = await browser.new_context()\n",
        "        page = await context.new_page()\n",
        "        await page.goto(DATABASE_URL)\n",
        "        database_html = await page.content() # Get the html of database\n",
        "        with open(HTML_SAVE_PATH + DATABASE_NAME + '.html', 'w', encoding=ENCODING_FORMAT) as f:\n",
        "            f.write(database_html) # Save the html of database as 'DATABASE_NAME.html'\n",
        "        await page.close()\n",
        "\n",
        "async def get_fraud_info(HTML_SAVE_PATH, DATABASE_NAME) -> None:\n",
        "    with open(HTML_SAVE_PATH + DATABASE_NAME + '.html', 'r', encoding=ENCODING_FORMAT) as f:\n",
        "        database_html_doc = f.read()\n",
        "    soup = BeautifulSoup(database_html_doc, 'html.parser')\n",
        "    all_url = set() # Use set to avoid the repetition of urls\n",
        "    main_content = soup.find('div', attrs={\"class\": MAIN_CONTENT_CLASS}) # Get the div in main-content because it contains all urls to be crawled\n",
        "    links = main_content.find_all('a')\n",
        "    for link in links:\n",
        "        all_url.add(link['href']) # Get all urls to be crawled\n",
        "    with open(HTML_SAVE_PATH+'all_urls_to_be_crawled'+'.html', 'w', encoding=ENCODING_FORMAT) as f:\n",
        "        f.write('\\n'.join(all_url)) # Save all urls to be crawled as 'all_urls_to_be_crawled.html', just for checking\n",
        "    with open(HTML_SAVE_PATH + DATABASE_NAME +'.csv', 'w', encoding=\"utf-8\", newline='') as csv_file:\n",
        "        writer = csv.writer(csv_file) # Create a csv file to save the fraud info\n",
        "        writer.writerow([\"URL\", \"タイトル\", \"詐欺種別\", \"名称等\", \"所在地\", \"代表者\", \"電話番号\", \"E-mail\", \"被害内容\"])\n",
        "        for fraud_info_link in list(all_url): # Crawl the fraud info from all urls\n",
        "            async with async_playwright() as playwright:\n",
        "                browser = await playwright.webkit.launch(headless=True)\n",
        "                context = await browser.new_context()\n",
        "                page = await context.new_page()\n",
        "                await page.goto(fraud_info_link)\n",
        "                fraud_info_html_content = await page.content()\n",
        "                soup = BeautifulSoup(fraud_info_html_content, 'html.parser')\n",
        "                fraud_info = soup.find('div', id='main-content') # Extract the fraud info from the html\n",
        "                # Get the fraud info\n",
        "                all_td_tag = fraud_info.find_all('td')\n",
        "                fraud_info_url = fraud_info_link\n",
        "                fraud_info_title = fraud_info.select_one('div.post-title > div:last-child').text\n",
        "                fraud_info_category = all_td_tag[0].text\n",
        "                fraud_info_name = all_td_tag[1].text\n",
        "                fraud_info_address = all_td_tag[2].text\n",
        "                fraud_info_representative = all_td_tag[3].text\n",
        "                fraud_info_phone_number = all_td_tag[4].text\n",
        "                fraud_info_email = all_td_tag[5].text\n",
        "                fraud_info_frauded_content = all_td_tag[7].text.replace('\\n', '\\\\n') # Replace '\\n' with '\\\\n' to avoid the error of inputting csv file\n",
        "                # Write the fraud info into csv file\n",
        "                writer.writerow([fraud_info_url, fraud_info_title, fraud_info_category, fraud_info_name, fraud_info_address, fraud_info_representative, fraud_info_phone_number, fraud_info_email, fraud_info_frauded_content])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(get_html_of_database(DATABASE_URL, DATABASE_NAME))\n",
        "    asyncio.run(get_fraud_info(HTML_SAVE_PATH, DATABASE_NAME))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
